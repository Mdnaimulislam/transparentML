{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions and problems\n",
    "\n",
    "### Import the required libraries and load datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load `Advertising`, `Credit`, and `Auto` datasets**\n",
    "\n",
    "Datasets available on https://www.statlearning.com/resources-first-edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "\n",
    "advertising_df = pd.read_csv(\n",
    "    data_url, header=0, index_col=0\n",
    ")  # read Boston data as pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_url = \"https://github.com/pykale/transparentML/raw/main/data/Credit.csv\"\n",
    "\n",
    "credit_df = pd.read_csv(credit_url)\n",
    "credit_df[\"Student2\"] = credit_df.Student.map({\"No\": 0, \"Yes\": 1})\n",
    "credit_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_url = \"https://github.com/pykale/transparentML/raw/main/data/Auto.csv\"\n",
    "\n",
    "auto_df = pd.read_csv(auto_url, na_values=\"?\").dropna()\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative predictors\n",
    "\n",
    "In the previous sections, the predictor variables are *quantitative*. For example, in the `Credit` dataset, the response is `Balance` (average credit card debt for each individual) and there are six quantitative predictors: `Age` , `Cards` (number of credit cards), `Education` (years of education), `Income` (in thousands of dollars), `Limit` (credit limit), and `Rating` (credit rating). The following code displays the pairwise relationships between these variables (Figure 3.6 in the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(\n",
    "    credit_df[[\"Balance\", \"Age\", \"Cards\", \"Education\", \"Income\", \"Limit\", \"Rating\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in practice, predictor variables are not always quantitative. For example in `Credit` dataset, it also contains four qualitative variables: `Student` (`Yes` or `No`), `Own` (`Yes` or `No`), `Married` (`Yes` or `No`), and `Region` (`South`, `East`, or `West`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictors with only two levels**\n",
    "\n",
    "Run the following Least squares coefficient estimates associated with the regression of `Balance` onto `Own` in the `Credit` data set (Table 3.7 in the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Balance ~ Own\", credit_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predictors with more than two levels**\n",
    "\n",
    "Run the following Least squares coefficient estimates associated with the regression of `Balance` onto `Region` in the `Credit` data set (Table 3.8 in the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Balance ~ Region\", credit_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions of linear regression\n",
    "\n",
    "#### Interaction between variables\n",
    "\n",
    "In the previous analysis of `Advertising` dataset, the predictor variables are assumed to be independent. However, this model may be incorrect. For example, `radio` advertising can increase the effectiveness of `TV` advertising, which is known as *interaction* effect in statistics. Consider the a standard multiple linear regression with two variables\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon.\n",
    "$$\n",
    "\n",
    "This model can be extended to include an interaction term between $X_1$ and $X_2$:\n",
    "\n",
    "\\begin{align}\n",
    "Y &= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2 + \\epsilon \\\\\n",
    "&= \\beta_0 + (\\beta_1 + \\beta_3 X_2) X_1 + \\beta_2 X_2 + \\epsilon \\\\\n",
    "&= \\beta_0 + \\tilde{\\beta}_1 X_1 + \\beta_2 X_2 + \\epsilon,\n",
    "\\end{align}\n",
    "\n",
    "where $\\tilde{\\beta}_1 = \\beta_1 + \\beta_3 X_2$ is the *effective* coefficient on $X_1$. Then the association between $Y$ and $X_1$ is no longer constant and depends on the value of $X_2$. Similarly, the association between $Y$ and $X_2$ is no longer constant and depends on the value of $X_1$.\n",
    "\n",
    "Run the following code for the `Advertising` data, least squares coefficient estimates associated with the regression of sales onto `TV` and `radio`, with an interaction term, `TV` $\\times$ `radio` (Table 3.9 of the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV + Radio + TV*Radio\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction between qualitative and quantitative variables\n",
    "\n",
    "The concept of interactions applies just as well to qualitative variables. In fact, sometimes an interaction between a qualitative variable and a quantitative variable has a particularly nice interpretation.\n",
    "\n",
    "Run the following code to predict `balance` using the `income` (quantitative) and `student` (qualitative) variables, and compare the differences between including and excluding the interaction term between `income` and `student` (Figure 3.7 of the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est1 = ols(\"Balance ~ Income + Student2\", credit_df).fit()\n",
    "est2 = ols(\"Balance ~ Income + Income*Student2\", credit_df).fit()\n",
    "\n",
    "print(\"Regression 1 - without interaction term\")\n",
    "print(est1.params)\n",
    "print(\"\\nRegression 2 - with interaction term\")\n",
    "print(est2.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income (x-axis)\n",
    "income = np.linspace(0, 150)\n",
    "\n",
    "# Balance without interaction term (y-axis)\n",
    "student1 = np.linspace(\n",
    "    regr1[\"Intercept\"] + regr1[\"Student2\"],\n",
    "    regr1[\"Intercept\"] + regr1[\"Student2\"] + 150 * regr1[\"Income\"],\n",
    ")\n",
    "non_student1 = np.linspace(\n",
    "    regr1[\"Intercept\"], regr1[\"Intercept\"] + 150 * regr1[\"Income\"]\n",
    ")\n",
    "\n",
    "# Balance with iteraction term (y-axis)\n",
    "student2 = np.linspace(\n",
    "    regr2[\"Intercept\"] + regr2[\"Student2\"],\n",
    "    regr2[\"Intercept\"]\n",
    "    + regr2[\"Student2\"]\n",
    "    + 150 * (regr2[\"Income\"] + regr2[\"Income:Student2\"]),\n",
    ")\n",
    "non_student2 = np.linspace(\n",
    "    regr2[\"Intercept\"], regr2[\"Intercept\"] + 150 * regr2[\"Income\"]\n",
    ")\n",
    "\n",
    "# Create plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax1.plot(income, student1, \"r\", income, non_student1, \"k\")\n",
    "ax2.plot(income, student2, \"r\", income, non_student2, \"k\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.legend([\"student\", \"non-student\"], loc=2)\n",
    "    ax.set_xlabel(\"Income\")\n",
    "    ax.set_ylabel(\"Balance\")\n",
    "    ax.set_ylim(ymax=1550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-linear relationships\n",
    "\n",
    "In some cases, the true relationship between the response and the predictors may be non-linear. Here we present a very simple way to directly extend the linear model to accommodate non-linear relationships, using *polynomial regression*.\n",
    "\n",
    "Run the following code to display the relationship between `mpg` (gas mileage in miles per gallon) and different degrees of `horsepower` (Figure 3.8 of the ISLR book) for a number of cars in the `Auto` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Seaborn's regplot() you can easily plot higher order polynomials.\n",
    "plt.scatter(\n",
    "    auto_df.horsepower, auto_df.mpg, facecolors=\"None\", edgecolors=\"k\", alpha=0.5\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Linear\",\n",
    "    scatter=False,\n",
    "    color=\"orange\",\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Degree 2\",\n",
    "    order=2,\n",
    "    scatter=False,\n",
    "    color=\"lightblue\",\n",
    ")\n",
    "sns.regplot(\n",
    "    x=auto_df.horsepower,\n",
    "    y=auto_df.mpg,\n",
    "    ci=None,\n",
    "    label=\"Degree 5\",\n",
    "    order=5,\n",
    "    scatter=False,\n",
    "    color=\"g\",\n",
    ")\n",
    "plt.legend()\n",
    "plt.ylim(5, 55)\n",
    "plt.xlim(40, 240);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure seem to suggest that a quadratic relationship between `mpg` and `horsepower` might be more appropriate than a linear relationship. We can fit such a model using the following regression:\n",
    "\n",
    "\\begin{equation}\n",
    "\\textrm{mpg} = \\beta_0 + \\beta_1 \\times \\textrm{horsepower} + \\beta_2 \\times \\textrm{horsepower}^2 + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "In fact, this is still a linear model with $X_1 = \\textrm{horsepower}$ and $X_2 = \\textrm{horsepower}^2$. Run the following code to fit a linear regression model with `mpg` as the response and `horsepower` and `horsepower^2` as the predictors (Table 3.10 of the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df[\"horsepower2\"] = auto_df.horsepower**2\n",
    "auto_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"mpg ~ horsepower + horsepower2\", auto_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems of linear regression\n",
    "\n",
    "- Non-linearity of the Data\n",
    "  \n",
    "  The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then virtually all of the conclusions that we draw from the fit are suspect. In addition, the prediction accuracy of the model can be significantly reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "# Linear fit\n",
    "X = auto_df.horsepower.values.reshape(-1, 1)\n",
    "y = auto_df.mpg\n",
    "regr.fit(X, y)\n",
    "\n",
    "auto_df[\"pred1\"] = regr.predict(X)\n",
    "auto_df[\"resid1\"] = auto_df.mpg - auto_df.pred1\n",
    "\n",
    "# Quadratic fit\n",
    "X2 = auto_df[[\"horsepower\", \"horsepower2\"]].values\n",
    "regr.fit(X2, y)\n",
    "\n",
    "auto_df[\"pred2\"] = regr.predict(X2)\n",
    "auto_df[\"resid2\"] = auto_df.mpg - auto_df.pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "sns.regplot(\n",
    "    x=auto_df.pred1,\n",
    "    y=auto_df.resid1,\n",
    "    lowess=True,\n",
    "    ax=ax1,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax1.hlines(\n",
    "    0,\n",
    "    xmin=ax1.xaxis.get_data_interval()[0],\n",
    "    xmax=ax1.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax1.set_title(\"Residual Plot for Linear Fit\")\n",
    "\n",
    "# Right plot\n",
    "sns.regplot(\n",
    "    x=auto_df.pred2,\n",
    "    y=auto_df.resid2,\n",
    "    lowess=True,\n",
    "    line_kws={\"color\": \"r\", \"lw\": 1},\n",
    "    ax=ax2,\n",
    "    scatter_kws={\"facecolors\": \"None\", \"edgecolors\": \"k\", \"alpha\": 0.5},\n",
    ")\n",
    "ax2.hlines(\n",
    "    0,\n",
    "    xmin=ax2.xaxis.get_data_interval()[0],\n",
    "    xmax=ax2.xaxis.get_data_interval()[1],\n",
    "    linestyles=\"dotted\",\n",
    ")\n",
    "ax2.set_title(\"Residual Plot for Quadratic Fit\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Fitted values\")\n",
    "    ax.set_ylabel(\"Residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collinearity\n",
    "  \n",
    "  Collinearity refers to the situation in which two or more predictor variables are closely related to one another. \n",
    "  \n",
    "  Run the following code to illustrate the problem of collinearity in the `Credit` dataset (Figure 3.14 of the ISLR book). In the left-hand panel, the two predictors `limit` and `age` appear to have no obvious relationship. In contrast, in the right-hand panel, the predictors `limit` and `rating` are very highly correlated with each other, and we say that they are *collinear*. In this context, `limit` and `rating` tend to increase or decrease together, it can be difficult to determine how each one separately is associated with the response, `balance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Left plot\n",
    "ax1.scatter(credit_df.Limit, credit_df.Age, facecolor=\"None\", edgecolor=\"r\")\n",
    "ax1.set_ylabel(\"Age\")\n",
    "\n",
    "# Right plot\n",
    "ax2.scatter(credit_df.Limit, credit_df.Rating, facecolor=\"None\", edgecolor=\"r\")\n",
    "ax2.set_ylabel(\"Rating\")\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(\"Limit\")\n",
    "    ax.set_xticks([2000, 4000, 6000, 8000, 12000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run following code to learn the difficulties that can result from collinearity in the context of the `Credit` dataset (Table 3.11 of the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = credit_df.Balance\n",
    "\n",
    "# Regression for left plot\n",
    "X = credit_df[[\"Age\", \"Limit\"]].values\n",
    "regr1 = LinearRegression()\n",
    "regr1.fit(scale(X.astype(\"float\"), with_std=False), y)\n",
    "print(\"Age/Limit\\n\", regr1.intercept_)\n",
    "print(regr1.coef_)\n",
    "\n",
    "# Regression for right plot\n",
    "X2 = credit_df[[\"Rating\", \"Limit\"]].values\n",
    "regr2 = LinearRegression()\n",
    "regr2.fit(scale(X2.astype(\"float\"), with_std=False), y)\n",
    "print(\"\\nRating/Limit\\n\", regr2.intercept_)\n",
    "print(regr2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid coordinates for plotting\n",
    "B_Age = np.linspace(regr1.coef_[0] - 3, regr1.coef_[0] + 3, 100)\n",
    "B_Limit = np.linspace(regr1.coef_[1] - 0.02, regr1.coef_[1] + 0.02, 100)\n",
    "\n",
    "B_Rating = np.linspace(regr2.coef_[0] - 3, regr2.coef_[0] + 3, 100)\n",
    "B_Limit2 = np.linspace(regr2.coef_[1] - 0.2, regr2.coef_[1] + 0.2, 100)\n",
    "\n",
    "X1, Y1 = np.meshgrid(B_Limit, B_Age, indexing=\"xy\")\n",
    "X2, Y2 = np.meshgrid(B_Limit2, B_Rating, indexing=\"xy\")\n",
    "Z1 = np.zeros((B_Age.size, B_Limit.size))\n",
    "Z2 = np.zeros((B_Rating.size, B_Limit2.size))\n",
    "\n",
    "Limit_scaled = scale(credit_df.Limit.astype(\"float\"), with_std=False)\n",
    "Age_scaled = scale(credit_df.Age.astype(\"float\"), with_std=False)\n",
    "Rating_scaled = scale(credit_df.Rating.astype(\"float\"), with_std=False)\n",
    "\n",
    "# Calculate Z-values (RSS) based on grid of coefficients\n",
    "for (i, j), v in np.ndenumerate(Z1):\n",
    "    Z1[i, j] = (\n",
    "        (y - (regr1.intercept_ + X1[i, j] * Limit_scaled + Y1[i, j] * Age_scaled)) ** 2\n",
    "    ).sum() / 1000000\n",
    "\n",
    "for (i, j), v in np.ndenumerate(Z2):\n",
    "    Z2[i, j] = (\n",
    "        (y - (regr2.intercept_ + X2[i, j] * Limit_scaled + Y2[i, j] * Rating_scaled))\n",
    "        ** 2\n",
    "    ).sum() / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 5))\n",
    "fig.suptitle(\"RSS - Regression coefficients\", fontsize=20)\n",
    "\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax2 = fig.add_subplot(122)\n",
    "\n",
    "min_RSS = r\"$\\beta_0$, $\\beta_1$ for minimized RSS\"\n",
    "\n",
    "# Left plot\n",
    "CS = ax1.contour(X1, Y1, Z1, cmap=plt.cm.Set1, levels=[21.25, 21.5, 21.8])\n",
    "ax1.scatter(regr1.coef_[1], regr1.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax1.clabel(CS, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "ax1.set_ylabel(r\"$\\beta_{Age}$\", fontsize=17)\n",
    "\n",
    "# Right plot\n",
    "CS = ax2.contour(X2, Y2, Z2, cmap=plt.cm.Set1, levels=[21.5, 21.8])\n",
    "ax2.scatter(regr2.coef_[1], regr2.coef_[0], c=\"r\", label=min_RSS)\n",
    "ax2.clabel(CS, inline=True, fontsize=10, fmt=\"%1.1f\")\n",
    "ax2.set_ylabel(r\"$\\beta_{Rating}$\", fontsize=17)\n",
    "ax2.set_xticks([-0.1, 0, 0.1, 0.2])\n",
    "\n",
    "for ax in fig.axes:\n",
    "    ax.set_xlabel(r\"$\\beta_{Limit}$\", fontsize=17)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left: A contour plot of RSS for the regression of `balance` onto `age` and `limit`. The minimum value is well defined. Right: A contour plot of RSS for the regression of `balance` onto `rating` and `limit`. Because of the collinearity, there are many pairs ($\\beta_{\\text{Limit}}$ , $\\beta_{\\text{Rating}}$) with a similar value for RSS.\n",
    "\n",
    "Collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\\hat{\\beta}_j$ to grow. in the presence of collinearity, we may fail to reject $H_0$ : $\\hat{\\beta}_j = 0$. This means that the power of the hypothesis test—the probability of correctly power detecting a non-zero coefficient is reduced by collinearity.\n",
    "\n",
    "Run the following code to compare the coefficient estimates obtained from two separate multiple regression models, where the first model is a regression of `balance` on `age` and `limit`, and the second model is a regression of `balance` on `rating` and `limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_age_limit = ols(\"Balance ~ Age + Limit\", credit_df).fit()\n",
    "est_age_limit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_rating_limit = ols(\"Balance ~ Rating + Limit\", credit_df).fit()\n",
    "\n",
    "# est_limit = ols(\"Limit ~ Age + Rating\", credit_df).fit()\n",
    "\n",
    "# print(1 / (1 - est_rating.rsquared))\n",
    "# print(1 / (1 - est_age.rsquared))\n",
    "# print(1 / (1 - est_limit.rsquared))\n",
    "\n",
    "est_rating_limit.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret the results? \n",
    ":class: tip, dropdown\n",
    "In the first regression, both `age` and `limit` are highly significant with very small p-values. In the second, the collinearity between `limit` and `rating` has caused the standard error for the `limit` coefficient estimate to increase by a factor of 12 and the p-value to increase to 0.701. In other words, the importance of the `limit` variable has been masked due to the presence of collinearity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This question involves the use of simple linear regression on the **Auto** dataset from [Tabel 1.2](https://pykale.github.io/transparentML/01-intro/organisation.html#datasets-table).\n",
    "\n",
    "    (a) Use the **LinearRegression()** function to perform a simple linear regression with **mpg** as the response and **horsepower** as the predictor. Use the **summary()** function to print the results. Comment on the output.\n",
    "    \n",
    "      For example:\n",
    "\n",
    "      >i. Is there a relationship between the predictor and the response?  \n",
    "\n",
    "      >ii. How strong is the relationship between the predictor and the response? \n",
    "\n",
    "      >iii. Is the relationship between the predictor and the response positive or negative? \n",
    "\n",
    "      >iv. What is the predicted __mpg__ associated with a **horsepower** of 98? What are the associated 95% confidence and prediction intervals?\n",
    "        \n",
    "    (b) Plot the response and predictor. Use the **regplot()** function to display the least-squared regression line.\n",
    "    \n",
    "    (c) Produce the diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit. \n",
    "\n",
    "   \n",
    "2. This question involves the use of multiple linear regression on the **Carseats** dataset from Tabel 1.2.\n",
    "\n",
    "    (a) Fit a multiple regression model to predict **Sales** using **Price**, **Urban**, and **US**.\n",
    "    \n",
    "    (b) Provide an interpretation of each coefficient in the model. Be careful--some of the variables in the model are qualitative!       \n",
    "    \n",
    "    (c) Write out the model in equation form, being careful to handle the qualitative variables properly. \n",
    "    \n",
    "    (d) For which of the predictors can you reject the null hypothesis $H_0 : \\beta_j = 0$ ?  \n",
    "    \n",
    "    (e) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\n",
    "    \n",
    "    (f) How well do the models in (a) and (e) fit the data?\n",
    "    \n",
    "    (g) Using the model from (e), obtain 95% confidence intervals for the coefficient(s).\n",
    "    \n",
    "    (h) Is there evidence of outliers or high-leverage observations in the model from (e)?  \n",
    "\n",
    "  \n",
    "3. This question involves the **Boston** dataset from [Tabel 1.2](https://pykale.github.io/transparentML/01-intro/organisation.html#datasets-table). We will now try to predict per capita crime rate using the other variables in this dataset. In other owrds, per capita crime rate is the response, and the other variables are the prediction.\n",
    "\n",
    "    (a) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\n",
    "    \n",
    "    (b) Fit a multiple regression model to predict the response using all of the predictors. Describe your results.. FOr which predictiors can we reject the null hypothesis $H_0 : \\beta_j = 0$ ? \n",
    "    \n",
    "    (c) How do your results from (a) compare to your results form (b)? Create a plot displaying the univariate regression coefficients from (a) on the x-axis, and the multiple regression coefficients from (b) on the y-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis.\n",
    "    \n",
    "    (d) Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form, \n",
    "    \n",
    "    <center> $$Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon$$\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "86bebae5cc7cea36980a1c5f8c0c180bdb90db2354d6524c00550a599e718092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
