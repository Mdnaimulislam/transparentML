{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "We have examined the relationship between `sale` and `TV` of the `Advertising` dataset for simple linear regression. There are two more predictor variables `Radio` and `Newspaper` in the dataset. How can we account for the effect of these two variables in the model? \n",
    "\n",
    "Watch the video below to learn about multiple linear regression.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/zITIFTsivN8?start=21\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Explaining Multiple Linear Regression, by StatQuest](https://www.youtube.com/embed/zITIFTsivN8?start=21)\n",
    "```\n",
    "\n",
    "Then read the following sections to learn more about multiple linear regression with examples in the ISLR book.\n",
    "\n",
    "### Import the required libraries and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../standard_import.txt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn-white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Datasets**\n",
    "\n",
    "Load `Advertising` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://github.com/pykale/transparentML/raw/main/data/Advertising.csv\"\n",
    "\n",
    "advertising_df = pd.read_csv(\n",
    "    data_url, header=0, index_col=0\n",
    ")  # read Boston data as pandas data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accommodate multiple predictor variables, one option is to run simple linear regression separately for each predictor variable. \n",
    "\n",
    "The following code run a simple linear regression model of `Radio`, and `Newspaper` onto `Sales` using `statsmodels`, respectively (Table 3.3 in the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ Radio\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ Newspaper\", advertising_df).fit()\n",
    "est.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, fitting a separate simple linear regression model for each predictor is not very efficient. It is unclear how to make a single prediction, while the effect of other predictors is ignored. A better approach is to use multiple linear regression. Multiple linear regression is an extension of simple linear regression. It allows us to predict a quantitative response using more than one predictor variable. The equation for a multiple linear regression model with two predictor variables is given by:\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $Y$ is the response, $X_1, X_2, ..., X_p$ are the predictors, and $\\epsilon$ is the error term. The $\\beta$'s are called the regression coefficients. The $\\beta_0$ is the intercept, and the $\\beta_1, \\beta_2, ..., \\beta_p$ are the slopes. The equation can be written in matrix form as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y} = \\mathbf{X} \\mathbf{\\beta} + \\boldsymbol{\\epsilon},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{Y}$ is a $n \\times 1$ vector of responses, $\\mathbf{X}$ is a $n \\times (p+1)$ matrix of predictors, $\\mathbf{\\beta}$ is a $(p+1) \\times 1$ vector of regression coefficients, and $\\boldsymbol{\\epsilon}$ is a $n \\times 1$ vector of errors. The $\\mathbf{X}$ matrix contains a column of 1's to account for the intercept. The $\\mathbf{\\beta}$ vector contains the intercept in the first position and the slopes for the remaining $p$ predictors. The $\\mathbf{\\epsilon}$ vector contains the error terms for each observation. Using the `Advertising` dataset as an example, we can fit a multiple linear regression model:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Sales} = \\beta_0 + \\beta_1 \\text{TV} + \\beta_2 \\times \\text{Radio} + \\beta_3 \\times \\text{Newspaper} + \\epsilon.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the regression coefficients\n",
    "\n",
    "Similar to simple linear regression, we can estimate the regression coefficients using least squares. The least squares estimates for the regression coefficients are given by:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{aligned}\n",
    "\\text{RSS} = & \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 \\\\\n",
    "= & \\sum_{i=1}^n (y_i - \\hat{\\beta}_0 - \\hat{\\beta}_1 x_{i1} - \\hat{\\beta}_2 x_{i2} - ... - \\hat{\\beta}_p x_{ip})^2,\n",
    "\\end{aligned}\n",
    "\\end{align}\n",
    "\n",
    "where $y_i$ is the $i$th response, $\\hat{y}_i$ is the $i$th predicted response, $\\hat{\\beta}_0$ is the intercept, $\\hat{\\beta}_1$ is the slope for $x_{i,1}$, $\\hat{\\beta}_2$ is the slope for $x_{i,2}$, and so on. The $\\hat{\\beta}$'s are the least squares estimates for the regression coefficients. The least squares estimates for the regression coefficients in matrix form are given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{Y}.\n",
    "\\end{equation}\n",
    "\n",
    "The following code run a multiple linear regression model of `TV`, `Radio`, and `Newspaper` onto `Sales` using `statsmodels`, and display the learnt coefficients (Table 3.4 in the ISLR book)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ols(\"Sales ~ TV + Radio + Newspaper\", advertising_df).fit()\n",
    "est.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret the results? \n",
    ":class: tip, dropdown\n",
    "We interpret these results as follows: for a given amount of `TV` and `newspaper `advertising, spending an additional \\$1,000 on `radio` advertising is associated with approximately 189 units of additional `sales`. Comparing these coefficient estimates to those displayed in Tables for simple linear regression, we notice that the multiple regression coefficient estimates for `TV` and `radio` are pretty similar to the simple linear regression coefficient estimates. However, while the `newspaper` regression coefficient estimate in simple linear regression was significantly non-zero, the coefficient estimate for `newspaper` in the multiple regression model is close to zero, and the corresponding p-value is no longer significant, with a value around 0.86. This illustrates that the simple and multiple regression coefficient can be quite different. This difference stems from the fact that in the simple regression case, the slope term represents the average increase in product sales associated with a \\$1,000 increase in newspaper advertising, ignoring other predictors such as `TV` and `radio`. By contrast, in the multiple regression setting, the coefficient for `newspaper` represents the average increase in product `sales` associated with increasing `newspaper` spending by \\$1,000 while holding `TV` and `radio` fixed.\n",
    "```\n",
    "\n",
    "Why the relationship between `sales` and `newspaper` are opposite in the simple linear regression and multiple linear regression? Use following code displays the correlation matrix of the `Advertising` dataset for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "advertising_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} How to interpret the results?\n",
    ":class: tip, dropdown\n",
    "This indicates that markets with high `newspaper` advertising tend to also have high `radio` advertising. Now suppose that the multiple regression is correct and `newspaper` advertising is not associated with sales, but `radio` advertising is associated with `sales`. Then in markets where we spend more on `radio` our sales will tend to be higher, and as our correlation matrix shows, we also tend to spend more on newspaper advertising in those same markets. Hence, in a simple linear regression which only examines sales versus `newspaper`, we will observe that higher values of `newspaper` tend to be associated with higher values of `sales` , even though newspaper advertising is not directly associated with sales. So newspaper advertising is a surrogate for `radio` advertising; `newspaper` gets “credit” for the association between `radio` on `sales`.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import questions in multiple linear regression\n",
    "\n",
    "\n",
    "1. Is at least one of the predictors $X_1, X_2, ..., X_p$ useful in predicting the response?\n",
    "2. Do all the predictors help to explain $Y$, or is only a subset of the predictors useful?\n",
    "3. How well does the model fit the data?\n",
    "4. Given a set of predictor values, what response value should we predict, and how accurate is our prediction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: We can answer this question by testing the null hypothesis that all the regression coefficients are zero, i.e.\n",
    "\n",
    "$$\n",
    "H_0: \\beta_1 = \\beta_2 = ... = \\beta_p = 0.\n",
    "$$\n",
    "\n",
    "versus the alternative hypothesis:\n",
    "\n",
    "$$\n",
    "H_a: \\text{at least one of the regression coefficients is non-zero}. \n",
    "$$ \n",
    "\n",
    "This hypothesis test is performed by computing the $F$-statistic, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "F = \\frac{(\\text{TSS} - \\text{RSS})/p}{\\text{RSS}/(n-p-1)},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{TSS} = \\sum(y_i - \\bar{y})^2$ and $\\text{RSS} = \\sum(y_i - \\hat{y}_i)^2$. When there is no relationship between the response and predictors, one would expect the $F$-statistic to take on a value close to 1. On the other hand, if $H_a$ a is true, we can expect $F$ to be greater than 1. \n",
    "\n",
    "The $F$-statistic for the multiple linear regression model obtained by regressing `sales` onto `radio`, `TV`, and `newspaper` is 570.3 (displayed above). Since this is far larger than 1, it provides compelling evidence against the null hypothesis $H_0$. In other words, the large $F$-statistic suggests that at least one of the advertising media must be related to `sales`.\n",
    "\n",
    "Watch the following video to learn more about the $F$-statistic.\n",
    "\n",
    "```{admonition} Video\n",
    "<iframe width=\"700\" height=\"394\" src=\"https://www.youtube.com/embed/nk2CQITm_eo?start=969&end=1525\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe> \n",
    "\n",
    "[Calculating a p-value for $R^2$, by StatQuest](https://www.youtube.com/embed/nk2CQITm_eo?start=969&end=1525)\n",
    "```\n",
    "\n",
    "<!-- then $E\\{\\text{TSS} - \\text{RSS}/p\\} > \\sigma^2$, where $\\sigma$ is the standard deviation of the error term, so we expect $F$ to be greater than 1. -->\n",
    "\n",
    "<!-- The numerator of the F-statistic is the ratio of the total sum of squares to the residual sum of squares, and the denominator is the ratio of the residual sum of squares to the degrees of freedom. The F-statistic has an $F$ distribution with $p$ and $n-p-1$ degrees of freedom. The null hypothesis is that all the regression coefficients are zero, and the alternative hypothesis is that at least one of the regression coefficients is non-zero.  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: It is more often that the response is only associated with a subset of the predictors. The task of determining which predictors are associated with the response, in order to fit a single model involving only those predictors, is referred to as *variable selection* (or *feature selection*). This will be discussed extensively in {doc}`Feature Selection and Shrinkage <../06-ftr-select-shrink/overview>`.\n",
    "\n",
    "There are three common approaches to variable selection:\n",
    "\n",
    "- Forward selection. We begin with a model containing no predictors, and then consider adding predictors one at a time until all predictors have been considered. The best single predictor is added to the model, and the process is repeated until all predictors have been added to the model. This is a greedy algorithm, and it is not guaranteed to find the best model containing a subset of the predictors. \n",
    "- Backward selection. We begin with a model containing all predictors, and then consider removing predictors one at a time until no predictors remain. The worst single predictor is removed from the model, and the process is repeated until no predictors remain in the model. This is also a greedy algorithm, and it is not guaranteed to find the best model containing a subset of the predictors.\n",
    "- Mixed selection. We begin with some initial model containing a subset of the predictors. We then consider adding or removing each predictor individually, and retain the best model that results. This is also a greedy algorithm, and it is not guaranteed to find the best model containing a subset of the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: Model fit. We can answer this question by computing the $R^2$ statistic, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "R^2 = 1 - \\frac{\\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}.\n",
    "\\end{equation}\n",
    "\n",
    "The $R^2$ statistic provides an indication of the proportion of the variance in the response that is predictable from the predictors. In this case, the $R^2$ statistic is 0.897, which indicates that 89.7% of the variance in `sales` is predictable from `TV`, `radio`, and `newspaper`.\n",
    "\n",
    "Q4: We can answer this question by computing the standard error of the regression, which is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-p-1}} = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{n-p-1}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "\n",
    "X = advertising_df[[\"Radio\", \"TV\"]].values\n",
    "y = advertising_df.Sales\n",
    "\n",
    "regr.fit(X, y)\n",
    "print(regr.coef_)\n",
    "print(regr.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the min/max values of Radio & TV?\n",
    "# Use these values to set up the grid for plotting.\n",
    "advertising_df[[\"Radio\", \"TV\"]].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a coordinate grid\n",
    "Radio = np.arange(0, 50)\n",
    "TV = np.arange(0, 300)\n",
    "\n",
    "B1, B2 = np.meshgrid(Radio, TV, indexing=\"xy\")\n",
    "Z = np.zeros((TV.size, Radio.size))\n",
    "\n",
    "for (i, j), v in np.ndenumerate(Z):\n",
    "    Z[i, j] = regr.intercept_ + B1[i, j] * regr.coef_[0] + B2[i, j] * regr.coef_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.suptitle(\"Regression: Sales ~ Radio + TV Advertising\", fontsize=20)\n",
    "\n",
    "ax = axes3d.Axes3D(fig, auto_add_to_figure=False)\n",
    "fig.add_axes(ax)\n",
    "\n",
    "ax.plot_surface(B1, B2, Z, rstride=10, cstride=5, alpha=0.4)\n",
    "ax.scatter3D(advertising_df.Radio, advertising_df.TV, advertising_df.Sales, c=\"r\")\n",
    "\n",
    "ax.set_xlabel(\"Radio\")\n",
    "ax.set_xlim(0, 50)\n",
    "ax.set_ylabel(\"TV\")\n",
    "ax.set_ylim(ymin=0)\n",
    "ax.set_zlabel(\"Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Predictions\n",
    "\n",
    "Once we have fit a multiple linear regression model, we can use the model to make predictions of the response for a given set of predictor values. \n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p.\n",
    "\\end{equation}\n",
    "\n",
    "However, we must be careful when making predictions, because the observed values of the predictors may not have been part of the data used to fit the model. In this case, the prediction may not be very accurate. We use a confidence interval to quantify the uncertainty associated with the prediction. \n",
    "\n",
    "<!-- \\begin{equation}\n",
    "\\hat{y} \\pm t_{n-p-1, 1-\\alpha/2} \\text{RSE} \\sqrt{1 + \\frac{1}{n} + \\frac{(x - \\bar{x})^2}{\\sum(x_i - \\bar{x})^2}}.\n",
    "\\end{equation} -->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pykale')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "86bebae5cc7cea36980a1c5f8c0c180bdb90db2354d6524c00550a599e718092"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
